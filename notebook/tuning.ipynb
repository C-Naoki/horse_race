{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータチューニング\n",
    "optunaを用いたパラメータ調整を、各変数の内容を勉強しつつ丁寧に行う。\n",
    "\n",
    "[詳細はこちら](https://qiita.com/c60evaporator/items/351188110f328ff921b9#%E3%83%99%E3%82%A4%E3%82%BA%E6%9C%80%E9%81%A9%E5%8C%96bayesianoptimization%E3%81%AE%E5%A0%B4%E5%90%88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ひとつ上のディレクトリをパスに追加\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"/Users/naoki/git/Horse-Racing/Notebook/plot.ipynb\"), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.module.preparing import TableMerger\n",
    "from backend.module.simulation import RankPredictor\n",
    "from backend.module.repository import (\n",
    "    Peds,\n",
    "    Result,\n",
    "    Race,\n",
    "    HorseProfile,\n",
    "    RaceCard,\n",
    "    OriginalRaceCard,\n",
    "    Refund,\n",
    ")\n",
    "from backend.module.handler import (\n",
    "    PredictionBrancher,\n",
    "    PredictionExtractor,\n",
    "    RefundCalculator,\n",
    ")\n",
    "from backend.module.record import PredictionWriter\n",
    "\n",
    "from backend.environment.mapping import Mapping\n",
    "from backend.environment.columns import Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, learning_curve, validation_curve, cross_validate\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, r2_score, mean_squared_error, ndcg_score\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from urllib.request import urlopen\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merge last all datas by breeder_id: 100%|██████████| 718/718 [01:57<00:00,  6.12it/s] \n",
      "merge last all datas by owner_id: 100%|██████████| 718/718 [01:24<00:00,  8.46it/s] \n",
      "merge last all datas by trainer_id: 100%|██████████| 718/718 [01:28<00:00,  8.15it/s] \n",
      "merge last all datas by horse_id: 100%|██████████| 718/718 [00:26<00:00, 26.79it/s] \n",
      "merge last all datas by jockey_id: 100%|██████████| 718/718 [00:58<00:00, 12.20it/s] \n"
     ]
    }
   ],
   "source": [
    "table_merger = TableMerger()\n",
    "race_card = RaceCard()\n",
    "df = table_merger(race_card.preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merge p1  data: 100%|██████████| 496/496 [00:37<00:00, 13.26it/s] \n",
      "merge p2  data: 100%|██████████| 496/496 [00:30<00:00, 16.47it/s] \n",
      "merge p3  data: 100%|██████████| 496/496 [00:29<00:00, 16.60it/s] \n",
      "merge p4  data: 100%|██████████| 496/496 [00:33<00:00, 14.92it/s] \n",
      "merge p5  data: 100%|██████████| 496/496 [00:37<00:00, 13.33it/s] \n"
     ]
    }
   ],
   "source": [
    "# 必要なデータの準備\n",
    "# race_results(=r.data_c)の作成\n",
    "hr = c.HorseResults(_dat.horse_results[\"overall\"])\n",
    "p = c.Peds(_dat.ped_results[\"overall\"])\n",
    "r = c.Results(_dat.race_results[\"overall\"], hr, p, avg=False)\n",
    "\n",
    "# r.data_cをtrain, test, validの3種のデータに分割\n",
    "train, test = m.split_data(r.data_c)\n",
    "train, valid = m.split_data(train)\n",
    "\n",
    "# クエリデータの生成\n",
    "query_train = list()\n",
    "query_valid = list()\n",
    "query_test = list()\n",
    "for i in train.groupby(level=0):\n",
    "    query_train.append(len(i[1]))\n",
    "for i in valid.groupby(level=0):\n",
    "    query_valid.append(len(i[1]))\n",
    "for i in test.groupby(level=0):\n",
    "    query_test.append(len(i[1]))\n",
    "\n",
    "drop_list = ['rank_binary', 'rank_regression', 'rank_lambdarank', 'date', 'jockey_id', 'breeder_id', 'owner_id', 'trainer_id', 'birthday', 'horse_id']\n",
    "OBJECTIVE_VARIALBLE = 'rank_lambdarank'\n",
    "USE_EXPLANATORY = [col for col in r.data_c.columns if col not in drop_list]\n",
    "\n",
    "# train, valid, testを説明変数と目的変数に分割\n",
    "X_train = train.drop(drop_list+['odds'], axis=1)\n",
    "y_train = {'rank_binary': train['rank_binary'], 'rank_regression': train['rank_regression'], 'rank_lambdarank': train['rank_lambdarank']}\n",
    "X_valid = valid.drop(drop_list+['odds'], axis=1)\n",
    "y_valid = {'rank_binary': valid['rank_binary'], 'rank_regression': valid['rank_regression'], 'rank_lambdarank': valid['rank_lambdarank']}\n",
    "X_test = test.drop(drop_list, axis=1)\n",
    "y_test = {'rank_binary': test['rank_binary'], 'rank_regression': test['rank_regression'], 'rank_lambdarank': test['rank_lambdarank']}\n",
    "\n",
    "# lgbm用のデータに変形する\n",
    "lgb_train = {\n",
    "                'binary': lgb_o.Dataset(X_train.values, y_train['rank_binary'].values, group=query_train), \n",
    "                'regression': lgb_o.Dataset(X_train.values, y_train['rank_regression'].values, group=query_train),\n",
    "                'lambdarank': lgb_o.Dataset(X_train.values, y_train['rank_lambdarank'].values, group=query_train)\n",
    "            }\n",
    "\n",
    "lgb_valid = {\n",
    "                'binary': lgb_o.Dataset(X_valid.values, y_valid['rank_binary'].values, reference=lgb_train['binary'], group=query_valid), \n",
    "                'regression': lgb_o.Dataset(X_valid.values, y_valid['rank_regression'].values, reference=lgb_train['regression'], group=query_valid),\n",
    "                'lambdarank': lgb_o.Dataset(X_valid.values, y_valid['rank_lambdarank'].values, reference=lgb_train['lambdarank'], group=query_valid)\n",
    "            }\n",
    "\n",
    "# 乱数シード\n",
    "seed = 42\n",
    "# 評価指標をRMSEに指定\n",
    "# 詳細はこちら: https://qiita.com/c60evaporator/items/ca7eb70e1508d2ba5359#%E3%81%A9%E3%82%8C%E3%81%8C%E3%81%84%E3%81%84%E3%81%AE-1\n",
    "scoring = 'neg_root_mean_squared_error' \n",
    "# モデル作成\n",
    "# n_estimators: 木の数\n",
    "# boosting_type: アンサンブル学習を行う際、利用する型の選択(ex. gbdt, dart, rf, gossなど)\n",
    "lgb_clf_binary = lgb.LGBMClassifier(boosting_type='dart', objective='binary', random_state=seed, n_estimators=50)\n",
    "lgb_clf_regression = lgb.LGBMRegressor(boosting_type='dart', objective='regression', random_state=seed, n_estimators=50)\n",
    "lgb_clf_lambdarank = lgb.LGBMRanker(boosting_type='dart', objective='lambdarank', random_state=seed, n_estimators=50)\n",
    "\n",
    "ori_params_binary = {\n",
    "    'objective': 'binary',\n",
    "    'metric': \"binary_logloss\",\n",
    "    'feature_pre_filter': False,\n",
    "    'boosting_type': 'dart'\n",
    "}\n",
    "\n",
    "ori_params_regression = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'feature_pre_filter': False,\n",
    "    'boosting_type': 'dart'\n",
    "}\n",
    "\n",
    "ori_params_lambdarank = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': \"ndcg\",\n",
    "    'feature_pre_filter': False,\n",
    "    'boosting_type': 'dart',\n",
    "    'eval_at': [1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-25 13:47:48,845]\u001b[0m A new study created in memory with name: no-name-fd576a76-57a6-452e-887e-43e565a1a1fd\u001b[0m\n",
      "feature_fraction, val_score: -inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.602550:  14%|#4        | 1/7 [00:02<00:12,  2.01s/it]\u001b[32m[I 2022-04-25 13:47:50,885]\u001b[0m Trial 0 finished with value: 0.6025497651256432 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.6025497651256432.\u001b[0m\n",
      "feature_fraction, val_score: 0.602550:  14%|#4        | 1/7 [00:02<00:12,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.603333:  29%|##8       | 2/7 [00:04<00:10,  2.05s/it]\u001b[32m[I 2022-04-25 13:47:52,959]\u001b[0m Trial 1 finished with value: 0.6033331745261236 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 0.6033331745261236.\u001b[0m\n",
      "feature_fraction, val_score: 0.603333:  29%|##8       | 2/7 [00:04<00:10,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.606500:  43%|####2     | 3/7 [00:06<00:08,  2.06s/it]\u001b[32m[I 2022-04-25 13:47:55,040]\u001b[0m Trial 2 finished with value: 0.6065002084835566 and parameters: {'feature_fraction': 1.0}. Best is trial 2 with value: 0.6065002084835566.\u001b[0m\n",
      "feature_fraction, val_score: 0.606500:  43%|####2     | 3/7 [00:06<00:08,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.611934:  57%|#####7    | 4/7 [00:09<00:07,  2.46s/it]\u001b[32m[I 2022-04-25 13:47:58,096]\u001b[0m Trial 3 finished with value: 0.6119342349461339 and parameters: {'feature_fraction': 0.8}. Best is trial 3 with value: 0.6119342349461339.\u001b[0m\n",
      "feature_fraction, val_score: 0.611934:  57%|#####7    | 4/7 [00:09<00:07,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.611934:  71%|#######1  | 5/7 [00:13<00:05,  2.96s/it]\u001b[32m[I 2022-04-25 13:48:01,953]\u001b[0m Trial 4 finished with value: 0.6079252321913107 and parameters: {'feature_fraction': 0.4}. Best is trial 3 with value: 0.6119342349461339.\u001b[0m\n",
      "feature_fraction, val_score: 0.611934:  71%|#######1  | 5/7 [00:13<00:05,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.611934:  86%|########5 | 6/7 [00:16<00:02,  2.97s/it]\u001b[32m[I 2022-04-25 13:48:04,933]\u001b[0m Trial 5 finished with value: 0.6082126928655203 and parameters: {'feature_fraction': 0.5}. Best is trial 3 with value: 0.6119342349461339.\u001b[0m\n",
      "feature_fraction, val_score: 0.611934:  86%|########5 | 6/7 [00:16<00:02,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.611934: 100%|##########| 7/7 [00:19<00:00,  3.01s/it]\u001b[32m[I 2022-04-25 13:48:08,040]\u001b[0m Trial 6 finished with value: 0.610485146220083 and parameters: {'feature_fraction': 0.7}. Best is trial 3 with value: 0.6119342349461339.\u001b[0m\n",
      "feature_fraction, val_score: 0.611934: 100%|##########| 7/7 [00:19<00:00,  2.74s/it]\n",
      "num_leaves, val_score: 0.611934:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:   5%|5         | 1/20 [00:04<01:27,  4.61s/it]\u001b[32m[I 2022-04-25 13:48:12,665]\u001b[0m Trial 7 finished with value: 0.5916901560783709 and parameters: {'num_leaves': 256}. Best is trial 7 with value: 0.5916901560783709.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:   5%|5         | 1/20 [00:04<01:27,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  10%|#         | 2/20 [00:08<01:13,  4.06s/it]\u001b[32m[I 2022-04-25 13:48:16,344]\u001b[0m Trial 8 finished with value: 0.5949985994548249 and parameters: {'num_leaves': 176}. Best is trial 8 with value: 0.5949985994548249.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  10%|#         | 2/20 [00:08<01:13,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  15%|#5        | 3/20 [00:10<00:54,  3.23s/it]\u001b[32m[I 2022-04-25 13:48:18,577]\u001b[0m Trial 9 finished with value: 0.6007191617028 and parameters: {'num_leaves': 77}. Best is trial 9 with value: 0.6007191617028.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  15%|#5        | 3/20 [00:10<00:54,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  20%|##        | 4/20 [00:13<00:47,  2.96s/it]\u001b[32m[I 2022-04-25 13:48:21,126]\u001b[0m Trial 10 finished with value: 0.6056240142582489 and parameters: {'num_leaves': 42}. Best is trial 10 with value: 0.6056240142582489.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  20%|##        | 4/20 [00:13<00:47,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  25%|##5       | 5/20 [00:15<00:42,  2.81s/it]\u001b[32m[I 2022-04-25 13:48:23,657]\u001b[0m Trial 11 finished with value: 0.6108678523640182 and parameters: {'num_leaves': 7}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  25%|##5       | 5/20 [00:15<00:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  30%|###       | 6/20 [00:19<00:46,  3.34s/it]\u001b[32m[I 2022-04-25 13:48:28,051]\u001b[0m Trial 12 finished with value: 0.5973107212571027 and parameters: {'num_leaves': 146}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  30%|###       | 6/20 [00:20<00:46,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  35%|###5      | 7/20 [00:23<00:44,  3.46s/it]\u001b[32m[I 2022-04-25 13:48:31,740]\u001b[0m Trial 13 finished with value: 0.6007515607343069 and parameters: {'num_leaves': 96}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  35%|###5      | 7/20 [00:23<00:44,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  40%|####      | 8/20 [00:28<00:47,  3.94s/it]\u001b[32m[I 2022-04-25 13:48:36,726]\u001b[0m Trial 14 finished with value: 0.591702934447311 and parameters: {'num_leaves': 227}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  40%|####      | 8/20 [00:28<00:47,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  45%|####5     | 9/20 [00:32<00:42,  3.84s/it]\u001b[32m[I 2022-04-25 13:48:40,327]\u001b[0m Trial 15 finished with value: 0.6059690209126496 and parameters: {'num_leaves': 76}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  45%|####5     | 9/20 [00:32<00:42,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  50%|#####     | 10/20 [00:35<00:35,  3.59s/it]\u001b[32m[I 2022-04-25 13:48:43,352]\u001b[0m Trial 16 finished with value: 0.594358614025268 and parameters: {'num_leaves': 201}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  50%|#####     | 10/20 [00:35<00:35,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  55%|#####5    | 11/20 [00:36<00:26,  2.94s/it]\u001b[32m[I 2022-04-25 13:48:44,836]\u001b[0m Trial 17 finished with value: 0.6003942729866506 and parameters: {'num_leaves': 4}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  55%|#####5    | 11/20 [00:36<00:26,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  60%|######    | 12/20 [00:38<00:20,  2.50s/it]\u001b[32m[I 2022-04-25 13:48:46,331]\u001b[0m Trial 18 finished with value: 0.6056982031781029 and parameters: {'num_leaves': 14}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  60%|######    | 12/20 [00:38<00:20,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  65%|######5   | 13/20 [00:39<00:15,  2.23s/it]\u001b[32m[I 2022-04-25 13:48:47,934]\u001b[0m Trial 19 finished with value: 0.6031344472960279 and parameters: {'num_leaves': 62}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  65%|######5   | 13/20 [00:39<00:15,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  70%|#######   | 14/20 [00:43<00:15,  2.53s/it]\u001b[32m[I 2022-04-25 13:48:51,184]\u001b[0m Trial 20 finished with value: 0.601502114144407 and parameters: {'num_leaves': 123}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  70%|#######   | 14/20 [00:43<00:15,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  75%|#######5  | 15/20 [00:45<00:12,  2.49s/it]\u001b[32m[I 2022-04-25 13:48:53,571]\u001b[0m Trial 21 finished with value: 0.6061200464925455 and parameters: {'num_leaves': 49}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  75%|#######5  | 15/20 [00:45<00:12,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  80%|########  | 16/20 [00:48<00:10,  2.51s/it]\u001b[32m[I 2022-04-25 13:48:56,122]\u001b[0m Trial 22 finished with value: 0.6095381416768444 and parameters: {'num_leaves': 32}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  80%|########  | 16/20 [00:48<00:10,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  85%|########5 | 17/20 [00:50<00:07,  2.41s/it]\u001b[32m[I 2022-04-25 13:48:58,295]\u001b[0m Trial 23 finished with value: 0.6107527470984611 and parameters: {'num_leaves': 25}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  85%|########5 | 17/20 [00:50<00:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  90%|######### | 18/20 [00:53<00:05,  2.68s/it]\u001b[32m[I 2022-04-25 13:49:01,597]\u001b[0m Trial 24 finished with value: 0.5979365152842612 and parameters: {'num_leaves': 115}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  90%|######### | 18/20 [00:53<00:05,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934:  95%|#########5| 19/20 [00:54<00:02,  2.25s/it]\u001b[32m[I 2022-04-25 13:49:02,831]\u001b[0m Trial 25 finished with value: 0.5854843157086131 and parameters: {'num_leaves': 2}. Best is trial 11 with value: 0.6108678523640182.\u001b[0m\n",
      "num_leaves, val_score: 0.611934:  95%|#########5| 19/20 [00:54<00:02,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.611934: 100%|##########| 20/20 [00:57<00:00,  2.52s/it]\u001b[32m[I 2022-04-25 13:49:05,997]\u001b[0m Trial 26 finished with value: 0.6119342349461339 and parameters: {'num_leaves': 31}. Best is trial 26 with value: 0.6119342349461339.\u001b[0m\n",
      "num_leaves, val_score: 0.611934: 100%|##########| 20/20 [00:57<00:00,  2.90s/it]\n",
      "bagging, val_score: 0.611934:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  10%|#         | 1/10 [00:02<00:23,  2.60s/it]\u001b[32m[I 2022-04-25 13:49:08,609]\u001b[0m Trial 27 finished with value: 0.6060939354162316 and parameters: {'bagging_fraction': 0.6101580050739246, 'bagging_freq': 4}. Best is trial 27 with value: 0.6060939354162316.\u001b[0m\n",
      "bagging, val_score: 0.611934:  10%|#         | 1/10 [00:02<00:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  20%|##        | 2/10 [00:04<00:18,  2.33s/it]\u001b[32m[I 2022-04-25 13:49:10,748]\u001b[0m Trial 28 finished with value: 0.6021981466368652 and parameters: {'bagging_fraction': 0.5593145608899209, 'bagging_freq': 3}. Best is trial 27 with value: 0.6060939354162316.\u001b[0m\n",
      "bagging, val_score: 0.611934:  20%|##        | 2/10 [00:04<00:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  30%|###       | 3/10 [00:07<00:18,  2.58s/it]\u001b[32m[I 2022-04-25 13:49:13,636]\u001b[0m Trial 29 finished with value: 0.6061411036947238 and parameters: {'bagging_fraction': 0.9809408866072624, 'bagging_freq': 3}. Best is trial 29 with value: 0.6061411036947238.\u001b[0m\n",
      "bagging, val_score: 0.611934:  30%|###       | 3/10 [00:07<00:18,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  40%|####      | 4/10 [00:12<00:20,  3.36s/it]\u001b[32m[I 2022-04-25 13:49:18,182]\u001b[0m Trial 30 finished with value: 0.6099444375866251 and parameters: {'bagging_fraction': 0.9143478665404126, 'bagging_freq': 4}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934:  40%|####      | 4/10 [00:12<00:20,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  50%|#####     | 5/10 [00:15<00:16,  3.29s/it]\u001b[32m[I 2022-04-25 13:49:21,358]\u001b[0m Trial 31 finished with value: 0.6031370173602385 and parameters: {'bagging_fraction': 0.4226131464982031, 'bagging_freq': 3}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934:  50%|#####     | 5/10 [00:15<00:16,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  60%|######    | 6/10 [00:17<00:12,  3.01s/it]\u001b[32m[I 2022-04-25 13:49:23,825]\u001b[0m Trial 32 finished with value: 0.6055442153053437 and parameters: {'bagging_fraction': 0.9689762834828883, 'bagging_freq': 7}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934:  60%|######    | 6/10 [00:17<00:12,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  70%|#######   | 7/10 [00:20<00:08,  2.87s/it]\u001b[32m[I 2022-04-25 13:49:26,406]\u001b[0m Trial 33 finished with value: 0.5985276896376417 and parameters: {'bagging_fraction': 0.47818213459145975, 'bagging_freq': 3}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934:  70%|#######   | 7/10 [00:20<00:08,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  80%|########  | 8/10 [00:23<00:05,  2.83s/it]\u001b[32m[I 2022-04-25 13:49:29,163]\u001b[0m Trial 34 finished with value: 0.6033100655370963 and parameters: {'bagging_fraction': 0.8869263766575282, 'bagging_freq': 5}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934:  80%|########  | 8/10 [00:23<00:05,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934:  90%|######### | 9/10 [00:25<00:02,  2.55s/it]\u001b[32m[I 2022-04-25 13:49:31,082]\u001b[0m Trial 35 finished with value: 0.6027206213453612 and parameters: {'bagging_fraction': 0.9969018095966268, 'bagging_freq': 7}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934:  90%|######### | 9/10 [00:25<00:02,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.611934: 100%|##########| 10/10 [00:28<00:00,  2.68s/it]\u001b[32m[I 2022-04-25 13:49:34,049]\u001b[0m Trial 36 finished with value: 0.6044390615693449 and parameters: {'bagging_fraction': 0.6637560842888165, 'bagging_freq': 6}. Best is trial 30 with value: 0.6099444375866251.\u001b[0m\n",
      "bagging, val_score: 0.611934: 100%|##########| 10/10 [00:28<00:00,  2.80s/it]\n",
      "feature_fraction_stage2, val_score: 0.611934:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.611934:  17%|#6        | 1/6 [00:02<00:14,  2.87s/it]\u001b[32m[I 2022-04-25 13:49:36,929]\u001b[0m Trial 37 finished with value: 0.6070945809279006 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.6070945809279006.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.611934:  17%|#6        | 1/6 [00:02<00:14,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.611934:  33%|###3      | 2/6 [00:05<00:10,  2.63s/it]\u001b[32m[I 2022-04-25 13:49:39,396]\u001b[0m Trial 38 finished with value: 0.6049354161398868 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 37 with value: 0.6070945809279006.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.611934:  33%|###3      | 2/6 [00:05<00:10,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.611934:  50%|#####     | 3/6 [00:08<00:08,  2.79s/it]\u001b[32m[I 2022-04-25 13:49:42,372]\u001b[0m Trial 39 finished with value: 0.6061096223014171 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 0.6070945809279006.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.611934:  50%|#####     | 3/6 [00:08<00:08,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.611934:  67%|######6   | 4/6 [00:10<00:05,  2.56s/it]\u001b[32m[I 2022-04-25 13:49:44,605]\u001b[0m Trial 40 finished with value: 0.6096637246080463 and parameters: {'feature_fraction': 0.784}. Best is trial 40 with value: 0.6096637246080463.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.611934:  67%|######6   | 4/6 [00:10<00:05,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.611934:  83%|########3 | 5/6 [00:13<00:02,  2.64s/it]\u001b[32m[I 2022-04-25 13:49:47,364]\u001b[0m Trial 41 finished with value: 0.609501442959533 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 40 with value: 0.6096637246080463.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.611934:  83%|########3 | 5/6 [00:13<00:02,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.611934: 100%|##########| 6/6 [00:16<00:00,  2.67s/it]\u001b[32m[I 2022-04-25 13:49:50,110]\u001b[0m Trial 42 finished with value: 0.6070612803950635 and parameters: {'feature_fraction': 0.88}. Best is trial 40 with value: 0.6096637246080463.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.611934: 100%|##########| 6/6 [00:16<00:00,  2.68s/it]\n",
      "regularization_factors, val_score: 0.611934:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:   5%|5         | 1/20 [00:02<00:53,  2.82s/it]\u001b[32m[I 2022-04-25 13:49:52,938]\u001b[0m Trial 43 finished with value: 0.6119342349461339 and parameters: {'lambda_l1': 2.6620987324551556e-06, 'lambda_l2': 1.5259581514013937e-06}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:   5%|5         | 1/20 [00:02<00:53,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  10%|#         | 2/20 [00:05<00:46,  2.57s/it]\u001b[32m[I 2022-04-25 13:49:55,332]\u001b[0m Trial 44 finished with value: 0.6065704908367012 and parameters: {'lambda_l1': 0.0002560238862664386, 'lambda_l2': 0.005192175253869118}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  10%|#         | 2/20 [00:05<00:46,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  15%|#5        | 3/20 [00:07<00:44,  2.63s/it]\u001b[32m[I 2022-04-25 13:49:58,049]\u001b[0m Trial 45 finished with value: 0.6084519289059228 and parameters: {'lambda_l1': 0.004877298929145192, 'lambda_l2': 0.28700013991588025}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  15%|#5        | 3/20 [00:07<00:44,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  20%|##        | 4/20 [00:10<00:42,  2.64s/it]\u001b[32m[I 2022-04-25 13:50:00,704]\u001b[0m Trial 46 finished with value: 0.6118292036677158 and parameters: {'lambda_l1': 7.467696593628954e-08, 'lambda_l2': 0.21979408575600687}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  20%|##        | 4/20 [00:10<00:42,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  25%|##5       | 5/20 [00:12<00:33,  2.22s/it]\u001b[32m[I 2022-04-25 13:50:02,174]\u001b[0m Trial 47 finished with value: 0.6031026604390476 and parameters: {'lambda_l1': 0.0014791640067078385, 'lambda_l2': 4.494804140466442}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  25%|##5       | 5/20 [00:12<00:33,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  30%|###       | 6/20 [00:14<00:31,  2.24s/it]\u001b[32m[I 2022-04-25 13:50:04,437]\u001b[0m Trial 48 finished with value: 0.6078377985238818 and parameters: {'lambda_l1': 2.750570512912e-06, 'lambda_l2': 0.1309131154349376}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  30%|###       | 6/20 [00:14<00:31,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  35%|###5      | 7/20 [00:17<00:34,  2.62s/it]\u001b[32m[I 2022-04-25 13:50:07,866]\u001b[0m Trial 49 finished with value: 0.610626855783942 and parameters: {'lambda_l1': 0.00023101740063200202, 'lambda_l2': 1.8695675861996767e-08}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  35%|###5      | 7/20 [00:17<00:34,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  40%|####      | 8/20 [00:20<00:33,  2.77s/it]\u001b[32m[I 2022-04-25 13:50:10,945]\u001b[0m Trial 50 finished with value: 0.6045482929579667 and parameters: {'lambda_l1': 0.006857172604086125, 'lambda_l2': 5.007452055352276e-06}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  40%|####      | 8/20 [00:20<00:33,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  45%|####5     | 9/20 [00:24<00:34,  3.12s/it]\u001b[32m[I 2022-04-25 13:50:14,835]\u001b[0m Trial 51 finished with value: 0.6087609448724804 and parameters: {'lambda_l1': 3.6447174112816005e-07, 'lambda_l2': 0.00014454633583660596}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  45%|####5     | 9/20 [00:24<00:34,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  50%|#####     | 10/20 [00:29<00:34,  3.48s/it]\u001b[32m[I 2022-04-25 13:50:19,128]\u001b[0m Trial 52 finished with value: 0.6100930970034478 and parameters: {'lambda_l1': 0.0012216573634087802, 'lambda_l2': 1.3655151889959046e-08}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  50%|#####     | 10/20 [00:29<00:34,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  55%|#####5    | 11/20 [00:30<00:27,  3.02s/it]\u001b[32m[I 2022-04-25 13:50:21,094]\u001b[0m Trial 53 finished with value: 0.6043125175206573 and parameters: {'lambda_l1': 9.210500150557918, 'lambda_l2': 1.9313298620297543e-06}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  55%|#####5    | 11/20 [00:30<00:27,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  60%|######    | 12/20 [00:33<00:23,  2.91s/it]\u001b[32m[I 2022-04-25 13:50:23,771]\u001b[0m Trial 54 finished with value: 0.6087609448724804 and parameters: {'lambda_l1': 4.883132002574603e-08, 'lambda_l2': 0.00025396949581084387}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  60%|######    | 12/20 [00:33<00:23,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  65%|######5   | 13/20 [00:36<00:20,  2.95s/it]\u001b[32m[I 2022-04-25 13:50:26,802]\u001b[0m Trial 55 finished with value: 0.6119342349461339 and parameters: {'lambda_l1': 5.219854022323435e-06, 'lambda_l2': 1.1943396940796062e-06}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  65%|######5   | 13/20 [00:36<00:20,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  70%|#######   | 14/20 [00:39<00:18,  3.01s/it]\u001b[32m[I 2022-04-25 13:50:29,941]\u001b[0m Trial 56 finished with value: 0.6119342349461339 and parameters: {'lambda_l1': 9.418425032059104e-06, 'lambda_l2': 1.445042439111641e-06}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  70%|#######   | 14/20 [00:39<00:18,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  75%|#######5  | 15/20 [00:43<00:15,  3.13s/it]\u001b[32m[I 2022-04-25 13:50:33,343]\u001b[0m Trial 57 finished with value: 0.6119342349461339 and parameters: {'lambda_l1': 1.705335039444513e-05, 'lambda_l2': 1.912788704430424e-07}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  75%|#######5  | 15/20 [00:43<00:15,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  80%|########  | 16/20 [00:47<00:13,  3.39s/it]\u001b[32m[I 2022-04-25 13:50:37,338]\u001b[0m Trial 58 finished with value: 0.6119342349461339 and parameters: {'lambda_l1': 1.1783961939009636e-06, 'lambda_l2': 2.9491505306353596e-05}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  80%|########  | 16/20 [00:47<00:13,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  85%|########5 | 17/20 [00:50<00:10,  3.48s/it]\u001b[32m[I 2022-04-25 13:50:41,038]\u001b[0m Trial 59 finished with value: 0.6119342349461339 and parameters: {'lambda_l1': 1.2674349653865208e-08, 'lambda_l2': 3.3671283726338455e-07}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  85%|########5 | 17/20 [00:50<00:10,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  90%|######### | 18/20 [00:55<00:07,  3.90s/it]\u001b[32m[I 2022-04-25 13:50:45,927]\u001b[0m Trial 60 finished with value: 0.6087609448724804 and parameters: {'lambda_l1': 3.904269989154688e-05, 'lambda_l2': 7.703702254101276e-08}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  90%|######### | 18/20 [00:55<00:07,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934:  95%|#########5| 19/20 [00:58<00:03,  3.60s/it]\u001b[32m[I 2022-04-25 13:50:48,802]\u001b[0m Trial 61 finished with value: 0.6087609448724804 and parameters: {'lambda_l1': 5.938487646206022e-07, 'lambda_l2': 6.23921873054578e-05}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934:  95%|#########5| 19/20 [00:58<00:03,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.611934: 100%|##########| 20/20 [01:02<00:00,  3.54s/it]\u001b[32m[I 2022-04-25 13:50:52,212]\u001b[0m Trial 62 finished with value: 0.6098810454135044 and parameters: {'lambda_l1': 1.2750817267754717e-08, 'lambda_l2': 0.0023576509497337216}. Best is trial 43 with value: 0.6119342349461339.\u001b[0m\n",
      "regularization_factors, val_score: 0.611934: 100%|##########| 20/20 [01:02<00:00,  3.10s/it]\n",
      "min_data_in_leaf, val_score: 0.611934:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.611934:  20%|##        | 1/5 [00:02<00:09,  2.45s/it]\u001b[32m[I 2022-04-25 13:50:54,681]\u001b[0m Trial 63 finished with value: 0.6089693294682315 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.6089693294682315.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.611934:  20%|##        | 1/5 [00:02<00:09,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.611934:  40%|####      | 2/5 [00:04<00:07,  2.40s/it]\u001b[32m[I 2022-04-25 13:50:57,062]\u001b[0m Trial 64 finished with value: 0.6036065815328561 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.6089693294682315.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.611934:  40%|####      | 2/5 [00:04<00:07,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.611934:  60%|######    | 3/5 [00:08<00:06,  3.11s/it]\u001b[32m[I 2022-04-25 13:51:00,999]\u001b[0m Trial 65 finished with value: 0.6074153127420229 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.6089693294682315.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.611934:  60%|######    | 3/5 [00:08<00:06,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.611934:  80%|########  | 4/5 [00:11<00:02,  2.92s/it]\u001b[32m[I 2022-04-25 13:51:03,629]\u001b[0m Trial 66 finished with value: 0.6036065815328561 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.6089693294682315.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.611934:  80%|########  | 4/5 [00:11<00:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 98515, number of used features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.611934: 100%|##########| 5/5 [00:13<00:00,  2.78s/it]\u001b[32m[I 2022-04-25 13:51:06,149]\u001b[0m Trial 67 finished with value: 0.6017199133176284 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.6089693294682315.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.611934: 100%|##########| 5/5 [00:13<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "ori_params_lambdarank = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': \"ndcg\",\n",
    "    'feature_pre_filter': False,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'eval_at': [1000]\n",
    "}\n",
    "# パラメータ設定2(lambdarank)\n",
    "lgb_lambdarank_results={}\n",
    "lgb_clf_o_lambdarank = lgb_o.train(\n",
    "                        params=ori_params_lambdarank,\n",
    "                        train_set=lgb_train['lambdarank'],\n",
    "                        valid_sets=(lgb_train['lambdarank'], lgb_valid['lambdarank']),\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        verbose_eval=False,\n",
    "                        early_stopping_rounds=10,\n",
    "                        evals_result=lgb_lambdarank_results\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ設定2(binary)\n",
    "lgb_binary_results={}\n",
    "lgb_clf_o_binary = lgb_o.train(\n",
    "                        params=ori_params_binary,\n",
    "                        train_set=lgb_train['binary'],\n",
    "                        valid_sets=(lgb_train['binary'], lgb_valid['binary']),\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        verbose_eval=False,\n",
    "                        early_stopping_rounds=10,\n",
    "                        evals_result=lgb_binary_results\n",
    "                        )\n",
    "\n",
    "# パラメータ設定2(regression)\n",
    "lgb_regression_results={}\n",
    "lgb_clf_o_regression = lgb_o.train(\n",
    "                        params=ori_params_regression,\n",
    "                        train_set=lgb_train['regression'],\n",
    "                        valid_sets=(lgb_train['regression'], lgb_valid['regression']),\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        verbose_eval=False,\n",
    "                        early_stopping_rounds=10,\n",
    "                        evals_result=lgb_regression_results\n",
    "                        )\n",
    "\n",
    "# パラメータ設定2(lambdarank)\n",
    "lgb_lambdarank_results={}\n",
    "lgb_clf_o_lambdarank = lgb_o.train(\n",
    "                        params=ori_params_lambdarank,\n",
    "                        train_set=lgb_train['lambdarank'],\n",
    "                        valid_sets=(lgb_train['lambdarank'], lgb_valid['lambdarank']),\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        verbose_eval=False,\n",
    "                        early_stopping_rounds=10,\n",
    "                        evals_result=lgb_lambdarank_results\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRanker(bagging_fraction=1.0, bagging_freq=0, early_stopping_round=10,\n",
       "           eval_at=[1000], feature_fraction=0.5, feature_pre_filter=False,\n",
       "           lambda_l1=2.5175606453305353, lambda_l2=1.3485498741964842e-07,\n",
       "           metric='ndcg', min_child_samples=100, n_estimators=50,\n",
       "           num_iterations=1000, num_leaves=7, objective='lambdarank',\n",
       "           random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_lambdarank = lgb_clf_o_lambdarank.params\n",
    "lgb_clf_lambdarank.set_params(**best_params_lambdarank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, shuffle=True, random_state=seed)  # KFoldでクロスバリデーション分割指定\n",
    "fit_params_lambdarank = {\n",
    "                        'callbacks': [\n",
    "                                        lgb.early_stopping(\n",
    "                                        stopping_rounds=10, # 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\n",
    "                                        verbose=0)\n",
    "                                    ],  # 学習中のコマンドライン出力\n",
    "                        'eval_metric': 'rmse',  # early_stopping_roundsの評価指標\n",
    "                        'eval_set': [(X_train.values, y_train['rank_lambdarank'].values)]  # early_stopping_roundsの評価指標算出用データ\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  30 | elapsed:    6.8s remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    7.5s finished\n"
     ]
    }
   ],
   "source": [
    "train_sizes, train_scores, valid_scores = learning_curve(   \n",
    "                                                            estimator=lgb_clf_lambdarank,\n",
    "                                                            X=X_train.values, y=y_train['rank_lambdarank'].values,\n",
    "                                                            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                            fit_params=fit_params_lambdarank,\n",
    "                                                            cv=cv, scoring=scoring, n_jobs=-1, verbose=-1\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _learning_curve_lambdarank(extimator, X, y, group, train_sizes, rank_values={0: 10, 1: 5, 2: 3}, cv=0.3):\n",
    "    # X_trainのデータの何割かを用いたデータ(_X_train)から、X_testを予測し、的中率をグラフ化する。\n",
    "    # 学習データの利用率とスコアの関係性のプロットが目的\n",
    "    group_train = group[:int(len(group)*(1-cv))]\n",
    "    group_test = group[int(len(group)*(1-cv)):]\n",
    "    X_train = X[:sum(group_train)]\n",
    "    y_train = y[:sum(group_train)]\n",
    "    X_test = X[sum(group_train):]\n",
    "    y_test = y[sum(group_train):]\n",
    "    for rate in train_sizes:\n",
    "        _group_train = group_train[:int(len(group_train)*rate)]\n",
    "        _X_train = X_train[:sum(_group_train)]\n",
    "        _y_train = y_train[:sum(_group_train)]\n",
    "        extimator.fit(\n",
    "            _X_train.values,\n",
    "            _y_train.values,\n",
    "            group=_group_train,\n",
    "        )\n",
    "        predict = extimator.predict(X_test)\n",
    "        print(np.repeat(np.arange(len(group_test)), group_test))\n",
    "        print(len(predict), len(y_test), len(np.repeat(np.arange(len(group_test)), group_test)))\n",
    "        df = pd.DataFrame({\n",
    "                            'query_id': np.repeat(np.arange(len(group_test)), group_test),\n",
    "                            'predict': predict,\n",
    "                            'correct': y_test})\n",
    "        for race in df.groupby(level=0):\n",
    "            print(race)\n",
    "            race = race[1].sort_values('predict', ascending=False)\n",
    "            # for rank_idx, score in rank_values.items():\n",
    "            #     if race.iloc[rank_idx] == score:\n",
    "            #         valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in y_test['rank_lambdarank'].groupby(level=0):\n",
    "#     print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: best_params_lambdarank.pop('num_iterations')\n",
    "# except: pass\n",
    "# try: best_params_lambdarank.pop('early_stopping_round')\n",
    "# except: pass\n",
    "# lgb_clf_lambdarank = lgb.LGBMRanker(**best_params_lambdarank)\n",
    "# _learning_curve_lambdarank(lgb_clf_lambdarank, X_train, y_train['rank_lambdarank'], query_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  30 | elapsed:    6.0s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1abd33df0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 学習曲線\n",
    "# 最適パラメータを学習器にセット\n",
    "best_params_lambdarank = lgb_clf_o_lambdarank.params\n",
    "lgb_clf_lambdarank.set_params(**best_params_lambdarank)\n",
    "# 学習曲線の取得\n",
    "train_sizes, train_scores, valid_scores = learning_curve(   \n",
    "                                                            estimator=lgb_clf_lambdarank,\n",
    "                                                            X=X_train.values, \n",
    "                                                            y=y_train['rank_lambdarank'].values,\n",
    "                                                            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                            fit_params=fit_params_lambdarank,\n",
    "                                                            cv=cv, \n",
    "                                                            scoring=scoring, \n",
    "                                                            n_jobs=-1, \n",
    "                                                            verbose=-1\n",
    "                                                        )\n",
    "# 学習データ指標の平均±標準偏差を計算\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std  = np.std(train_scores, axis=1)\n",
    "train_center = train_mean\n",
    "train_high = train_mean + train_std\n",
    "train_low = train_mean - train_std\n",
    "# 検証データ指標の平均±標準偏差を計算\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std  = np.std(valid_scores, axis=1)\n",
    "valid_center = valid_mean\n",
    "valid_high = valid_mean + valid_std\n",
    "valid_low = valid_mean - valid_std\n",
    "# training_scoresをプロット\n",
    "plt.plot(train_sizes, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "plt.fill_between(train_sizes, train_high, train_low, alpha=0.15, color='blue')\n",
    "# validation_scoresをプロット\n",
    "plt.plot(train_sizes, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "plt.fill_between(train_sizes, valid_high, valid_low, alpha=0.15, color='green')\n",
    "# 最高スコアの表示\n",
    "best_score = valid_center[len(valid_center) - 1]\n",
    "plt.text(np.amax(train_sizes), valid_low[len(valid_low) - 1], f'best_score={best_score}', color='black', verticalalignment='top', horizontalalignment='right')\n",
    "# 軸ラベルおよび凡例の指定\n",
    "plt.xlabel('training examples')  # 学習サンプル数を横軸ラベルに\n",
    "plt.ylabel(scoring)  # スコア名を縦軸ラベルに\n",
    "plt.legend(loc='lower right')  # 凡例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 検証曲線\n",
    "# # 検証曲線描画対象パラメータ\n",
    "# valid_curve_params = {\n",
    "#                         'reg_alpha': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1],\n",
    "#                         'reg_lambda': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1],\n",
    "#                         'num_leaves': [2, 3, 4, 5, 6],\n",
    "#                         'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#                         'subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#                         'subsample_freq': [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "#                         'min_child_samples': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#                     }\n",
    "# param_scales = {\n",
    "#                 'reg_alpha': 'log',\n",
    "#                 'reg_lambda': 'log',\n",
    "#                 'num_leaves': 'linear',\n",
    "#                 'colsample_bytree': 'linear',\n",
    "#                 'subsample': 'linear',\n",
    "#                 'subsample_freq': 'linear',\n",
    "#                 'min_child_samples': 'linear'\n",
    "#                 }\n",
    "# # 最適パラメータを上記描画対象に追加\n",
    "# for k, v in valid_curve_params.items():\n",
    "#     if best_params[k] not in v:\n",
    "#         v.append(best_params[k])\n",
    "#         v.sort()\n",
    "# for i, (k, v) in enumerate(valid_curve_params.items()):\n",
    "#     # モデルに最適パラメータを適用\n",
    "#     lgb_clf_regression.set_params(**best_params)\n",
    "#     # 検証曲線を描画\n",
    "#     train_scores, valid_scores = validation_curve(\n",
    "#                                                     estimator=lgb_clf_regression,\n",
    "#                                                     X=X_train, y=y_train['rank_lambdarank'],\n",
    "#                                                     param_name=k,\n",
    "#                                                     param_range=v,\n",
    "#                                                     fit_params=fit_params,\n",
    "#                                                     cv=cv, scoring=scoring,\n",
    "#                                                     n_jobs=-1, verbose=-1)\n",
    "#     # 学習データに対するスコアの平均±標準偏差を算出\n",
    "#     train_mean = np.mean(train_scores, axis=1)\n",
    "#     train_std  = np.std(train_scores, axis=1)\n",
    "#     train_center = train_mean\n",
    "#     train_high = train_mean + train_std\n",
    "#     train_low = train_mean - train_std\n",
    "#     # テストデータに対するスコアの平均±標準偏差を算出\n",
    "#     valid_mean = np.mean(valid_scores, axis=1)\n",
    "#     valid_std  = np.std(valid_scores, axis=1)\n",
    "#     valid_center = valid_mean\n",
    "#     valid_high = valid_mean + valid_std\n",
    "#     valid_low = valid_mean - valid_std\n",
    "#     # training_scoresをプロット\n",
    "#     plt.plot(v, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "#     plt.fill_between(v, train_high, train_low, alpha=0.15, color='blue')\n",
    "#     # validation_scoresをプロット\n",
    "#     plt.plot(v, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "#     plt.fill_between(v, valid_high, valid_low, alpha=0.15, color='green')\n",
    "#     # 最適パラメータを縦線表示\n",
    "#     plt.axvline(x=best_params[k], color='gray')\n",
    "#     # スケールをparam_scalesに合わせて変更\n",
    "#     plt.xscale(param_scales[k])\n",
    "#     # 軸ラベルおよび凡例の指定\n",
    "#     plt.xlabel(k)  # パラメータ名を横軸ラベルに\n",
    "#     plt.ylabel(scoring)  # スコア名を縦軸ラベルに\n",
    "#     plt.legend(loc='lower right')  # 凡例\n",
    "#     # グラフを描画\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c05dc3219fcd543b1f0376570d45d347b84df25f0ea6f4909d168aaecceef9a6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('Horse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
